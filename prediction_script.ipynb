{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cPickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from fraud_detection_feature_eng import tokenize, prep_data\n",
    "import requests\n",
    "import json\n",
    "import ast\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Mongo Database and the table for storing fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gstudent/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: remove is deprecated. Use delete_one or delete_many instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.fraud_detector\n",
    "db.fraud_table.remove()\n",
    "table = db.fraud_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to insert new datapoints with associated probabilities to the MongoDB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_db(df, probabilities, table):\n",
    "    df_data = pd.concat(objs = [df, pd.DataFrame(probabilities, columns=['fraud_probability'])], axis=1)\n",
    "    df_data = df_data.T.to_dict().values()\n",
    "    table.insert(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to make predictions on new data gotten from http://galvanize-case-study-on-fraud.herokuapp.com/data_point\n",
    "A new datapoint is presented on that html page every 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    col_names = [u'approx_payout_date',u'body_length',u'channels',u'country',u'currency',u'delivery_method',u'description',u'email_domain',u'event_created',u'event_end',u'event_published',u'event_start',u'fb_published',u'gts',u'has_analytics',u'has_header'\n",
    "    ,u'has_logo',u'listed',u'name',\n",
    "    u'name_length',u'num_order',u'num_payouts',u'object_id',u'org_desc',u'org_facebook',u'org_name',u'org_twitter',u'payee_name',u'payout_type',u'previous_payouts',u'sale_duration',u'sale_duration2',u'show_map',u'ticket_types',u'user_age',u'user_created',u'user_type',u'venue_address',u'venue_country'\n",
    "    ,u'venue_latitude',u'venue_longitude',u'venue_name',u'venue_state']\n",
    "\n",
    "    html = requests.get('http://galvanize-case-study-on-fraud.herokuapp.com/data_point')\n",
    "    fraud_dict = html.json()\n",
    "\n",
    "    df = pd.DataFrame(columns=col_names)\n",
    "    df = df.append(fraud_dict, ignore_index=True)\n",
    "\n",
    "    df_, X_data = prep_data(df)\n",
    "    with open('pickle/model.pkl') as f:\n",
    "        model = pickle.load(f)\n",
    "    probabilities = model.predict_proba(X_data)[:,1]\n",
    "    feed_db(df, probabilities, table)\n",
    "\n",
    "    lisst_ = []\n",
    "    for i in db.fraud_table.find( {}, { 'org_name': 1, 'name': 1, 'event_start' : 1, 'event_created':1, 'country' : 1 ,'object_id':1, '_id':0, 'fraud_probability':1 }):\n",
    "        i = ast.literal_eval(json.dumps(i))\n",
    "        i['event_created'] = date.fromtimestamp(int(i['event_created'])).__str__()\n",
    "        i['event_start'] = date.fromtimestamp(int(i['event_start'])).__str__()\n",
    "        if i['fraud_probability'] > 0.9:\n",
    "            i['risk'] = 'High'\n",
    "            i['style'] = \"<td style='background-color: #FF9999'>\"\n",
    "        elif i['fraud_probability'] < 0.01:\n",
    "            i['risk'] = 'Low'\n",
    "            i['style'] = \"<td style='background-color: #ADDEAD'>\"\n",
    "        else:\n",
    "            i['risk'] = 'Medium'\n",
    "            i['style'] = \"<td style='background-color: #b0eaf6'>\"\n",
    "        lisst_.append(i)\n",
    "\n",
    "    lisst_1 = []\n",
    "    return lisst_[::-1], pd.concat(objs = [df_, pd.DataFrame(probabilities, columns=['fraud_probability'])], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
