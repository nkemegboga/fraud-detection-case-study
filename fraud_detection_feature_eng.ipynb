{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cPickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fraud = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the 'acct_type' column into a 'fraud' column with 1 for fraud and 0 for non-fraud events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraud = {'premium':0, 'spammer_limited': 0, 'spammer_warn':0, 'tos_warn':0,\n",
    "        'spammer_noinvite':0, 'tos_lock':0, 'locked':0, 'spammer_web':0,\n",
    "        'spammer':0, 'fraudster_event':1, 'fraudster':1, 'fraudster_att':1}\n",
    "df_fraud['fraud'] = df_fraud['acct_type'].map(fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an 'event_duration' column which is the the difference between the event start and end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fraud['event_duration'] = (df_fraud.event_end - df_fraud.event_start)/3600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of the top 30 most common domains and classify all other domains as 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_domains = set(pd.DataFrame(df_fraud.email_domain.value_counts()[0:31]).index)\n",
    "for i,row in enumerate(df_fraud.email_domain):\n",
    "    if row not in top_domains:\n",
    "        df_fraud.ix[i, 'email_domain'] = 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that a lot of 'payee_names' were missing and that most of the columns with missing payee_names corresponded to fraud events, so I classified 'payee_names' as 1 if a name was given and 0 if no name was given. The 'payout_type' column was also missing entries and I classified the 'payout_type' for these samples as 'not_available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_fraud.iterrows():\n",
    "    if len(row['payee_name']) == 0:\n",
    "        df_fraud.ix[index, 'payee_name'] = 0\n",
    "    else:\n",
    "        df_fraud.ix[index, 'payee_name'] = 1\n",
    "    if len(row['payout_type']) == 0:\n",
    "        df_fraud.ix[index, 'payout_type'] = 'not_available'\n",
    "df_fraud['payee_name'] = df_fraud['payee_name'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ticket_types' column was made up of dictionaries giving details about the different ticket types ordered for each event. Each ticket type dictionary had information on cost, quantity sold and total quantity available. From these dictionaries, I added columns to our dataframe to represent the count of ticket types offered, minimum price, maximum price, total quantity sold, total quantity of tickets available, value of sold tickets, total value of tickets available and a weighted price.\n",
    "\n",
    "The 'previous_payouts' column was also made up of dictionaries with information on previous events created by the user. From these dictionaries, I added columns to our dataframe to represent the count of previous events crated by the user and the average payout to the user per event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_fraud.iterrows():   \n",
    "    df_fraud.ix[index,'types_count'] = len(row['ticket_types'])\n",
    "    df_fraud.ix[index,'min_price'] = min([j['cost'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'max_price'] = max([j['cost'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'quantity_sold'] = sum([j['quantity_sold'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'quantity_total'] = sum([j['quantity_total'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'value_sold'] = sum([j['quantity_sold']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'value_total'] = sum([j['quantity_total']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "    \n",
    "    df_fraud.ix[index,'payout_count'] = len(row['previous_payouts'])\n",
    "    try:\n",
    "        df_fraud.ix[index,'avg_payout'] = sum([j['amount'] for j in row['previous_payouts']] or [0]) / len(row['previous_payouts'])\n",
    "    except:\n",
    "        df_fraud.ix[index,'avg_payout'] = 0    \n",
    "    \n",
    "    \n",
    "for index, row in df_fraud.iterrows():     \n",
    "    try:\n",
    "        df_fraud.ix[index,'weighted_price'] = row['value_total'] / row['quantity_total']\n",
    "    except:\n",
    "        df_fraud.ix[index,'weighted_price'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropped columns  with data that's no longer useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fraud = df_fraud.drop(['acct_type','event_end', 'event_start', 'object_id',\n",
    "                          'venue_name','venue_address', 'user_created',\n",
    "                         'venue_latitude', 'venue_longitude', 'ticket_types',\n",
    "                         'previous_payouts'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder for converting string classes into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode = {}\n",
    "for column in ['country', 'currency', 'email_domain', 'listed', 'payout_type',\n",
    "              'venue_country', 'venue_state']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df_fraud[column])\n",
    "    df_fraud[column] = le.transform(df_fraud[column])\n",
    "    encode[column] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit an NLP model to only the 'description', 'name', 'org_desc'; and 'org_name' columns\n",
    "The idea is to get a single probability number for each entry in these columns and plug that number into the overall dataframe in place of the text in those columns. The final model fit is pickled to be called and used to transform new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    '''\n",
    "    INPUT: string\n",
    "    OUTPUT: list of strings\n",
    "\n",
    "    Tokenize and stem/lemmatize the document.\n",
    "    '''\n",
    "    bs = BeautifulSoup(doc.encode('ascii','ignore'))\n",
    "    document = bs.text\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stops = stopwords.words('english')\n",
    "    sp = set(punctuation)\n",
    "    sp.add('``')\n",
    "    sp.add(\"''\")\n",
    "    texts = [word for word in document.lower().split() if word not in sp]\n",
    "    texts = [word for word in document if word not in stops]\n",
    "    texts = [wordnet_lemmatizer.lemmatize(word) for word in document]\n",
    "    return texts\n",
    "def getvals(text):\n",
    "    '''\n",
    "    INPUT: array of strings\n",
    "    OUTPUT: Tfidf matrix\n",
    "\n",
    "    Vecotrize the strings.\n",
    "    '''    \n",
    "    tf = TfidfVectorizer(tokenizer=tokenize,max_features=20000)\n",
    "    return tf.fit(text)\n",
    "arr1 = df_fraud['description'].values\n",
    "arr2 = df_fraud['name'].values\n",
    "arr3 = df_fraud['org_desc'].values\n",
    "arr4 = df_fraud['org_name'].values\n",
    "models = [getvals(arr1),getvals(arr2),getvals(arr3),getvals(arr4)]\n",
    "\n",
    "def getall(df, models):\n",
    "    '''\n",
    "    INPUT: dataframe and tfidf models\n",
    "    OUTPUT: fit random forest models\n",
    "    '''    \n",
    "    y = df['fraud'].values\n",
    "    arr1 = df['description'].values\n",
    "    arr2 = df['name'].values\n",
    "    arr3 = df['org_desc'].values\n",
    "    arr4 = df['org_name'].values\n",
    "    rf1 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    rf2 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    rf3 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    rf4 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    x1 = models[0].transform(arr1).toarray()\n",
    "    x2 = models[1].transform(arr2).toarray()\n",
    "    x3 = models[2].transform(arr3).toarray()\n",
    "    x4 = models[3].transform(arr4).toarray()\n",
    "    rf1.fit(x1,y)\n",
    "    rf2.fit(x2,y)\n",
    "    rf3.fit(x3,y)\n",
    "    rf4.fit(x4,y)\n",
    "    return rf1,rf2,rf3,rf4\n",
    "\n",
    "a,b,c,d = getall(df)\n",
    "names = ['description','name','org_desc','org_name']\n",
    "for i,model in enumerate(models):\n",
    "    with open('tfidf-'+names[i]+'.pkl','w') as f:\n",
    "        pickle.dump(model,f)\n",
    "        \n",
    "rf_models = [a,b,c,d]\n",
    "for i,model in enumerate(rf_models):\n",
    "    with open('rfmodel-'+names[i]+'.pkl','w') as f:\n",
    "        pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the pickled tfidf and random forest models and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickle/tfidf-description.pkl') as f:\n",
    "    tfidf_description = pickle.load(f)\n",
    "with open('pickle/rfmodel-description.pkl') as f:\n",
    "    rf_description = pickle.load(f)\n",
    "df_fraud['description']= rf_description.predict_proba(tfidf_description.transform(df_fraud['description']))[:,1]\n",
    "\n",
    "with open('pickle/tfidf-name.pkl') as f:\n",
    "    tfidf_name = pickle.load(f)\n",
    "with open('pickle/rfmodel-name.pkl') as f:\n",
    "    rf_name = pickle.load(f)\n",
    "df_fraud['name']= rf_name.predict_proba(tfidf_name.transform(df_fraud['name']))[:,1]\n",
    "\n",
    "with open('pickle/tfidf-org_name.pkl') as f:\n",
    "    tfidf_org_name = pickle.load(f)\n",
    "with open('pickle/rfmodel-org_name.pkl') as f:\n",
    "    rf_org_name = pickle.load(f)\n",
    "df_fraud['org_name']= rf_org_name.predict_proba(tfidf_org_name.transform(df_fraud['org_name']))[:,1]\n",
    "\n",
    "with open('pickle/tfidf-org_desc.pkl') as f:\n",
    "    tfidf_org_desc = pickle.load(f)\n",
    "with open('pickle/rfmodel-org_desc.pkl') as f:\n",
    "    rf_org_desc = pickle.load(f)\n",
    "df_fraud['org_desc']= rf_org_desc.predict_proba(tfidf_org_desc.transform(df_fraud['org_desc']))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the encoder and list of top domains to be used to transform new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickle/top_domains.pkl', 'wb') as f:\n",
    "    pickle.dump(top_domains, f)\n",
    "with open('pickle/encode.pkl', 'wb') as f:\n",
    "    pickle.dump(encode, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all the feature engineering steps above into one function that can be run on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_data(df_fraud):\n",
    "    \n",
    "    df_fraud['event_duration'] = (df_fraud.event_end - df_fraud.event_start)/3600.\n",
    "    \n",
    "    with open('pickle/top_domains.pkl', 'w') as f:\n",
    "        top_domains = pickle.load(f)\n",
    "    for i,row in enumerate(df_fraud.email_domain):\n",
    "        if row not in top_domains:\n",
    "            df_fraud.ix[i, 'email_domain'] = 'other'\n",
    "    \n",
    "    for index, row in df_fraud.iterrows():\n",
    "        if index != 0:\n",
    "            if len(row['payee_name']) == 0:\n",
    "                df_fraud.ix[index, 'payee_name'] = 0\n",
    "            else:\n",
    "                df_fraud.ix[index, 'payee_name'] = 1\n",
    "        if len(row['payout_type']) == 0:\n",
    "            df_fraud.ix[index, 'payout_type'] = 'not_available'\n",
    "    df_fraud['payee_name'] = df_fraud['payee_name'].astype(float)\n",
    "    \n",
    "    for index, row in df_fraud.iterrows():   \n",
    "        df_fraud.ix[index,'types_count'] = len(row['ticket_types'])\n",
    "        df_fraud.ix[index,'min_price'] = min([j['cost'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'max_price'] = max([j['cost'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'quantity_sold'] = sum([j['quantity_sold'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'quantity_total'] = sum([j['quantity_total'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'value_sold'] = sum([j['quantity_sold']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'value_total'] = sum([j['quantity_total']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "\n",
    "        df_fraud.ix[index,'payout_count'] = len(row['previous_payouts'])\n",
    "        try:\n",
    "            df_fraud.ix[index,'avg_payout'] = sum([j['amount'] for j in row['previous_payouts']] or [0]) / len(row['previous_payouts'])\n",
    "        except:\n",
    "            df_fraud.ix[index,'avg_payout'] = 0    \n",
    "    \n",
    "    \n",
    "    for index, row in df_fraud.iterrows():     \n",
    "        try:\n",
    "            df_fraud.ix[index,'weighted_price'] = row['value_total'] / row['quantity_total']\n",
    "        except:\n",
    "            df_fraud.ix[index,'weighted_price'] = 0\n",
    "            \n",
    "    df_fraud = df_fraud.drop(['acct_type','event_end', 'event_start', 'object_id',\n",
    "                          'venue_name','venue_address', 'user_created',\n",
    "                         'venue_latitude', 'venue_longitude', 'ticket_types',\n",
    "                         'previous_payouts'],axis=1)\n",
    "    \n",
    "    with open('pickle/encode.pkl', 'w') as f:\n",
    "        encode = pickle.load(f)\n",
    "    for column in ['country', 'currency', 'email_domain', 'listed', 'payout_type',\n",
    "              'venue_country', 'venue_state']:\n",
    "        df_fraud[column] = encode[column].transform(df_fraud[column])\n",
    "        \n",
    "    with open('pickle/tfidf-description.pkl') as f:\n",
    "        tfidf_description = pickle.load(f)\n",
    "    with open('pickle/rfmodel-description.pkl') as f:\n",
    "        rf_description = pickle.load(f)\n",
    "    df_fraud['description']= rf_description.predict_proba(tfidf_description.transform(df_fraud['description']))[:,1]\n",
    "    \n",
    "    with open('pickle/tfidf-name.pkl') as f:\n",
    "        tfidf_name = pickle.load(f)\n",
    "    with open('pickle/rfmodel-name.pkl') as f:\n",
    "        rf_name = pickle.load(f)\n",
    "    df_fraud['name']= rf_name.predict_proba(tfidf_name.transform(df_fraud['name']))[:,1]\n",
    "    \n",
    "    with open('pickle/tfidf-org_name.pkl') as f:\n",
    "        tfidf_org_name = pickle.load(f)\n",
    "    with open('pickle/rfmodel-org_name.pkl') as f:\n",
    "        rf_org_name = pickle.load(f)\n",
    "    df_fraud['org_name']= rf_org_name.predict_proba(tfidf_org_name.transform(df_fraud['org_name']))[:,1]\n",
    "    \n",
    "    with open('pickle/tfidf-org_desc.pkl') as f:\n",
    "        tfidf_org_desc = pickle.load(f)\n",
    "    with open('pickle/rfmodel-org_desc.pkl') as f:\n",
    "        rf_org_desc = pickle.load(f)\n",
    "    df_fraud['org_desc']= rf_org_desc.predict_proba(tfidf_org_desc.transform(df_fraud['org_desc']))[:,1]\n",
    "    \n",
    "    return df_fraud, df_fraud.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break down dataframe into y (the fraud column) and X (all other columns after feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = df_fraud.pop('fraud').values\n",
    "X_data = df_fraud.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split with 25% of data held out for model scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit an XGBoost model to the training data and score the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_train)[:,1]\n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the XGBoost moel to be called and used to make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickle/model.pkl', 'w') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('pickle/probabilities.pkl', 'w') as f:\n",
    "    pickle.dump(probabilities, f)\n",
    "with open('pickle/actual.pkl', 'w') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix to evaluate the effect of thresholds on false positive rate, accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "probs = model.predict_proba(X_train)[:,1]\n",
    "for i in np.linspace(0.0001, 0.9999, 30):\n",
    "    preds = [1 if j > i else 0 for j in probs]\n",
    "    print confusion_matrix(y_train, preds)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
