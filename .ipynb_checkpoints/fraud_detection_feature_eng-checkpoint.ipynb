{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cPickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fraud = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the 'acct_type' column into a 'fraud' column with 1 for fraud and 0 for non-fraud events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraud = {'premium':0, 'spammer_limited': 0, 'spammer_warn':0, 'tos_warn':0,\n",
    "        'spammer_noinvite':0, 'tos_lock':0, 'locked':0, 'spammer_web':0,\n",
    "        'spammer':0, 'fraudster_event':1, 'fraudster':1, 'fraudster_att':1}\n",
    "df_fraud['fraud'] = df_fraud['acct_type'].map(fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an 'event_duration' column which is the the difference between the event start and end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fraud['event_duration'] = (df_fraud.event_end - df_fraud.event_start)/3600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of the top 30 most common domains and classify all other domains as 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_domains = set(pd.DataFrame(df_fraud.email_domain.value_counts()[0:31]).index)\n",
    "for i,row in enumerate(df_fraud.email_domain):\n",
    "    if row not in top_domains:\n",
    "        df_fraud.ix[i, 'email_domain'] = 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that a lot of 'payee_names' were missing and that most of the columns with missing payee_names corresponded to fraud events, so I classified 'payee_names' as 1 if a name was given and 0 if no name was given. The 'payout_type' column was also missing entries and I classified the 'payout_type' for these samples as 'not_available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_fraud.iterrows():\n",
    "    if len(row['payee_name']) == 0:\n",
    "        df_fraud.ix[index, 'payee_name'] = 0\n",
    "    else:\n",
    "        df_fraud.ix[index, 'payee_name'] = 1\n",
    "    if len(row['payout_type']) == 0:\n",
    "        df_fraud.ix[index, 'payout_type'] = 'not_available'\n",
    "df_fraud['payee_name'] = df_fraud['payee_name'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ticket_types' column was made up of dictionaries giving details about the different ticket types ordered for each event. Each ticket type dictionary had information on cost, quantity sold and total quantity available. From these dictionaries, I added columns to our dataframe to represent the count of ticket types offered, minimum price, maximum price, total quantity sold, total quantity of tickets available, value of sold tickets, total value of tickets available and a weighted price.\n",
    "\n",
    "The 'previous_payouts' column was also made up of dictionaries with information on previous events created by the user. From these dictionaries, I added columns to our dataframe to represent the count of previous events crated by the user and the average payout to the user per event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_fraud.iterrows():   \n",
    "    df_fraud.ix[index,'types_count'] = len(row['ticket_types'])\n",
    "    df_fraud.ix[index,'min_price'] = min([j['cost'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'max_price'] = max([j['cost'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'quantity_sold'] = sum([j['quantity_sold'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'quantity_total'] = sum([j['quantity_total'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'value_sold'] = sum([j['quantity_sold']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "    df_fraud.ix[index,'value_total'] = sum([j['quantity_total']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "    \n",
    "    df_fraud.ix[index,'payout_count'] = len(row['previous_payouts'])\n",
    "    try:\n",
    "        df_fraud.ix[index,'avg_payout'] = sum([j['amount'] for j in row['previous_payouts']] or [0]) / len(row['previous_payouts'])\n",
    "    except:\n",
    "        df_fraud.ix[index,'avg_payout'] = 0    \n",
    "    \n",
    "    \n",
    "for index, row in df_fraud.iterrows():     \n",
    "    try:\n",
    "        df_fraud.ix[index,'weighted_price'] = row['value_total'] / row['quantity_total']\n",
    "    except:\n",
    "        df_fraud.ix[index,'weighted_price'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fraud = df_fraud.drop(['acct_type','event_end', 'event_start', 'object_id',\n",
    "                          'venue_name','venue_address', 'user_created',\n",
    "                         'venue_latitude', 'venue_longitude', 'ticket_types',\n",
    "                         'previous_payouts'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode = {}\n",
    "for column in ['country', 'currency', 'email_domain', 'listed', 'payout_type',\n",
    "              'venue_country', 'venue_state']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df_fraud[column])\n",
    "    df_fraud[column] = le.transform(df_fraud[column])\n",
    "    encode[column] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    bs = BeautifulSoup(doc.encode('ascii','ignore'))\n",
    "    document = bs.text\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stops = stopwords.words('english')\n",
    "    sp = set(punctuation)\n",
    "    sp.add('``')\n",
    "    sp.add(\"''\")\n",
    "    texts = [word for word in document.lower().split() if word not in sp]\n",
    "    texts = [word for word in texts if word not in stops]\n",
    "    texts = [wordnet_lemmatizer.lemmatize(word) for word in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gstudent/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/gstudent/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "with open('pickle/tfidf-description.pkl') as f:\n",
    "    tfidf_description = pickle.load(f)\n",
    "with open('pickle/rfmodel-description.pkl') as f:\n",
    "    rf_description = pickle.load(f)\n",
    "df_fraud['description']= rf_description.predict_proba(tfidf_description.transform(df_fraud['description']))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickle/tfidf-name.pkl') as f:\n",
    "    tfidf_name = pickle.load(f)\n",
    "with open('pickle/rfmodel-name.pkl') as f:\n",
    "    rf_name = pickle.load(f)\n",
    "df_fraud['name']= rf_name.predict_proba(tfidf_name.transform(df_fraud['name']))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gstudent/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.flagshipsd.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/gstudent/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.aucklanddyspraxia.org.nz\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "with open('pickle/tfidf-org_name.pkl') as f:\n",
    "    tfidf_org_name = pickle.load(f)\n",
    "with open('pickle/rfmodel-org_name.pkl') as f:\n",
    "    rf_org_name = pickle.load(f)\n",
    "df_fraud['org_name']= rf_org_name.predict_proba(tfidf_org_name.transform(df_fraud['org_name']))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gstudent/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://montreal.shambhala.org/2011_2012/program/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "with open('pickle/tfidf-org_desc.pkl') as f:\n",
    "    tfidf_org_desc = pickle.load(f)\n",
    "with open('pickle/rfmodel-org_desc.pkl') as f:\n",
    "    rf_org_desc = pickle.load(f)\n",
    "df_fraud['org_desc']= rf_org_desc.predict_proba(tfidf_org_desc.transform(df_fraud['org_desc']))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickle/top_domains.pkl', 'wb') as f:\n",
    "    pickle.dump(top_domains, f)\n",
    "with open('pickle/encode.pkl', 'wb') as f:\n",
    "    pickle.dump(encode, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = df_fraud.pop('fraud').values\n",
    "X_data = df_fraud.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_data(df_fraud):\n",
    "    \n",
    "    df_fraud['event_duration'] = (df_fraud.event_end - df_fraud.event_start)/3600.\n",
    "    \n",
    "    with open('pickle/top_domains.pkl', 'w') as f:\n",
    "        top_domains = pickle.load(f)\n",
    "    for i,row in enumerate(df_fraud.email_domain):\n",
    "        if row not in top_domains:\n",
    "            df_fraud.ix[i, 'email_domain'] = 'other'\n",
    "    \n",
    "    for index, row in df_fraud.iterrows():\n",
    "        if index != 0:\n",
    "            if len(row['payee_name']) == 0:\n",
    "                df_fraud.ix[index, 'payee_name'] = 0\n",
    "            else:\n",
    "                df_fraud.ix[index, 'payee_name'] = 1\n",
    "        if len(row['payout_type']) == 0:\n",
    "            df_fraud.ix[index, 'payout_type'] = 'not_available'\n",
    "    df_fraud['payee_name'] = df_fraud['payee_name'].astype(float)\n",
    "    \n",
    "    for index, row in df_fraud.iterrows():   \n",
    "        df_fraud.ix[index,'types_count'] = len(row['ticket_types'])\n",
    "        df_fraud.ix[index,'min_price'] = min([j['cost'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'max_price'] = max([j['cost'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'quantity_sold'] = sum([j['quantity_sold'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'quantity_total'] = sum([j['quantity_total'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'value_sold'] = sum([j['quantity_sold']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "        df_fraud.ix[index,'value_total'] = sum([j['quantity_total']*j['cost'] for j in row['ticket_types']] or [0])\n",
    "\n",
    "        df_fraud.ix[index,'payout_count'] = len(row['previous_payouts'])\n",
    "        try:\n",
    "            df_fraud.ix[index,'avg_payout'] = sum([j['amount'] for j in row['previous_payouts']] or [0]) / len(row['previous_payouts'])\n",
    "        except:\n",
    "            df_fraud.ix[index,'avg_payout'] = 0    \n",
    "    \n",
    "    \n",
    "    for index, row in df_fraud.iterrows():     \n",
    "        try:\n",
    "            df_fraud.ix[index,'weighted_price'] = row['value_total'] / row['quantity_total']\n",
    "        except:\n",
    "            df_fraud.ix[index,'weighted_price'] = 0\n",
    "            \n",
    "    df_fraud = df_fraud.drop(['acct_type','event_end', 'event_start', 'object_id',\n",
    "                          'venue_name','venue_address', 'user_created',\n",
    "                         'venue_latitude', 'venue_longitude', 'ticket_types',\n",
    "                         'previous_payouts'],axis=1)\n",
    "    \n",
    "    with open('pickle/encode.pkl', 'w') as f:\n",
    "        encode = pickle.load(f)\n",
    "    for column in ['country', 'currency', 'email_domain', 'listed', 'payout_type',\n",
    "              'venue_country', 'venue_state']:\n",
    "        df_fraud[column] = encode[column].transform(df_fraud[column])\n",
    "        \n",
    "    with open('pickle/tfidf-description.pkl') as f:\n",
    "        tfidf_description = pickle.load(f)\n",
    "    with open('pickle/rfmodel-description.pkl') as f:\n",
    "        rf_description = pickle.load(f)\n",
    "    df_fraud['description']= rf_description.predict_proba(tfidf_description.transform(df_fraud['description']))[:,1]\n",
    "    \n",
    "    with open('pickle/tfidf-name.pkl') as f:\n",
    "        tfidf_name = pickle.load(f)\n",
    "    with open('pickle/rfmodel-name.pkl') as f:\n",
    "        rf_name = pickle.load(f)\n",
    "    df_fraud['name']= rf_name.predict_proba(tfidf_name.transform(df_fraud['name']))[:,1]\n",
    "    \n",
    "    with open('pickle/tfidf-org_name.pkl') as f:\n",
    "        tfidf_org_name = pickle.load(f)\n",
    "    with open('pickle/rfmodel-org_name.pkl') as f:\n",
    "        rf_org_name = pickle.load(f)\n",
    "    df_fraud['org_name']= rf_org_name.predict_proba(tfidf_org_name.transform(df_fraud['org_name']))[:,1]\n",
    "    \n",
    "    with open('pickle/tfidf-org_desc.pkl') as f:\n",
    "        tfidf_org_desc = pickle.load(f)\n",
    "    with open('pickle/rfmodel-org_desc.pkl') as f:\n",
    "        rf_org_desc = pickle.load(f)\n",
    "    df_fraud['org_desc']= rf_org_desc.predict_proba(tfidf_org_desc.transform(df_fraud['org_desc']))[:,1]\n",
    "    \n",
    "    return df_fraud, df_fraud.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "# model.predict_proba(X_test)\n",
    "# model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickle/model.pkl', 'w') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('pickle/probabilities.pkl', 'w') as f:\n",
    "    pickle.dump(probabilities, f)\n",
    "with open('pickle/actual.pkl', 'w') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "probs = model.predict_proba(X_train)[:,1]\n",
    "for i in np.linspace(0.0001, 0.9999, 30):\n",
    "    preds = [1 if j > i else 0 for j in probs]\n",
    "    print confusion_matrix(y_train, preds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'acct_type',\n",
       " u'approx_payout_date',\n",
       " u'body_length',\n",
       " u'channels',\n",
       " u'country',\n",
       " u'currency',\n",
       " u'delivery_method',\n",
       " u'description',\n",
       " u'email_domain',\n",
       " u'event_created',\n",
       " u'event_end',\n",
       " u'event_published',\n",
       " u'event_start',\n",
       " u'fb_published',\n",
       " u'gts',\n",
       " u'has_analytics',\n",
       " u'has_header',\n",
       " u'has_logo',\n",
       " u'listed',\n",
       " u'name',\n",
       " u'name_length',\n",
       " u'num_order',\n",
       " u'num_payouts',\n",
       " u'object_id',\n",
       " u'org_desc',\n",
       " u'org_facebook',\n",
       " u'org_name',\n",
       " u'org_twitter',\n",
       " u'payee_name',\n",
       " u'payout_type',\n",
       " u'previous_payouts',\n",
       " u'sale_duration',\n",
       " u'sale_duration2',\n",
       " u'show_map',\n",
       " u'ticket_types',\n",
       " u'user_age',\n",
       " u'user_created',\n",
       " u'user_type',\n",
       " u'venue_address',\n",
       " u'venue_country',\n",
       " u'venue_latitude',\n",
       " u'venue_longitude',\n",
       " u'venue_name',\n",
       " u'venue_state']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_fraud.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data_test.json', 'w') as f:\n",
    "    json.dump(X_test.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_fraud_test = pd.read_json('data.json')\n",
    "y_data_ = df_fraud_test.pop('acct_type').values\n",
    "X_data_ = df_fraud.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_, y_data_, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
